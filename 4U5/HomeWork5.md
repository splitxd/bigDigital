# АНАЛИЗ ДАННЫХ В РАЗРАБОТКЕ ИГР [in GameDev]
Отчет по лабораторной работе #5 выполнила:
- Коробейникова Анастасия Денисовна
- НМТ-231805

Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Найдите внутри C# скрипта “коэффициент корреляции”.
- Задание 2.
- Изменить параметры файла yaml-агента и определить какие параметры и как влияют на 
обучение модели. Привести описание не менее трех параметров.
- Задание 3.
- Приведите примеры.
- Выводы.

## Цель работы
Изучить ML Agent. Привести примеры, изучить скрипт и yaml файлы.


## Задание 0
### Ход работы

#### Часть 1
![packeges](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /packeges.png)
![start](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /start.png)
![multi](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /multi.png)
![script](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /script.png)
![resultroller](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /resultroller.png)
![withlearn](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /withlearn.png)
_____________________________________________________________________________________________

#### Часть 2
![starteco](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /starteco.png)
![miners](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /economics.png)
![resulteco](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /resulteco.png)
![graphs](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /graphs.png)

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции”
В этом случае коэффициент корреляции это tempInf.
tempInf — это процентное изменение цены между двумя месяцами

![corr](https://github.com/ ТВОЙ РЕПОЗИТОРИЙ /corr.png)

#### Влияние на обучение:
1.Если tempInf низкий (≤6):

-Агент получает положительную награду, что стимулирует повторять текущую стратегию.
Это закрепляет успешные действия, например, оптимальные значения speedMove, timeMining, 
amountGold, pickaxeCost, и profitPercentage.

-Способствует быстрому завершению эпизодов и эффективному обучению.

2.Если tempInf высокий (>6):

-Агент получает штраф, что заставляет его искать новые стратегии, чтобы улучшить результат.

-Модель адаптируется и пытается минимизировать нестабильность цен между месяцами.

## Задание 2
###  файл yaml-агента

#### batch_size (размер батча)

Что это: Это количество данных, которые используются за один раз, чтобы обновить модель.

Как влияет:

-Если увеличить: Модель будет обучаться более точно, так как изменения основываются на большем количестве данных. Но это требует больше памяти и может замедлить обучение.

-Если уменьшить: Обучение станет быстрее, так как обновления происходят чаще. Однако результаты могут быть менее стабильными, а модель может переобучиться.

#### learning_rate (скорость обучения)

Что это: Это то, насколько сильно модель изменяет свои настройки, чтобы уменьшить ошибку.

Как влияет:

-Если увеличить: Обучение будет идти быстрее, но модель может стать нестабильной и пропустить лучшее решение.

-Если уменьшить: Модель будет учиться более плавно и стабильно, но процесс займёт больше времени.

#### hidden_units (нейроны в скрытых слоях)

Что это: Это количество элементов, которые обрабатывают данные в каждом слое модели.

Как влияет:

-Если увеличить: Модель сможет находить сложные закономерности в данных, но потребуется больше ресурсов, и обучение может стать медленнее.

-Если уменьшить: Модель станет проще и быстрее, но может не справляться со сложными задачами.

## Задание 3
### Приведите примеры

Примеры первого ML Agent:

-Обучение беспилотных машин разворачиваться с учетом угла поворота и ограничения местности.

-Обучение игровых NPC нахождения кротчайшего пути до близжайшего waypoint'а , если они сбились с пути.

Примеры второго ML Agent:

-Регулирование цен в игре перед ее выходом, оценивая рыночную стоимость добываемого товара 
относительно скорости и простоты добычи.

-Регулирование получения опыта в играх с убийством моснстров, которая будет зависеть от 
редкости встречи с монстром и сложностью его убийства.

#### В каких случаях проще использовать ML-агент, а не писать программную реализацию решения

В перспективе долгосрочного обслуживания и изменения. Будет проще регулировать , например, экономику имея 
предстваление об ее изменении построенном на ошибках, скачках и удачных решениях. Так же они будут актуальны
в обучении ИИ, развивая их , где обычный код явно бы не сработал.

## Выводы

Были применены модели ML Agentов , применены на практике, сделаны предположения об актуальности и 
пременении их в действительности.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
